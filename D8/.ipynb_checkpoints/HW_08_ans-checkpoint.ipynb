{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import HalvingRandomSearchCV as HRSCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from project_module import regression_report\n",
    "from project_module.feature_selection import SelectKBestByCoefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 89) (1095,) (365, 89) (365,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "x_train = np.load('./x_train.npy')\n",
    "y_train = np.load('./y_train.npy')\n",
    "x_test = np.load('./x_test.npy')\n",
    "y_test = np.load('./y_test.npy')\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解題步驟：\n",
    "\n",
    "1. 讀取 x_train.npy, y_train.npy, x_test.npy, y_test.npy\n",
    "2. 先以上課的知識或 default hyperparameter 調整出一個不會 over-fitting 太多的 XGBoost 模型\n",
    "3. 以該組超參數為基準，搜尋附近的參數(可以用自己偏好的搜尋策略)\n",
    "4. 將最終調整結果與一開始的模型做比較，誤差是否有降低\n",
    "5. 請比較 Random Forest, XGBoost(有時間的同學可以增加 GBDT, Adaboost) 的超參數搜尋時間與誤差(記得要控制 n_estimators 等會影響到時間的參數，使其叫)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XGBoost(params: dict) -> XGBRegressor:\n",
    "    XGB = XGBRegressor(**params)\n",
    "    return XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse = 1577792384.2319\n",
      "mae = 18800.7493\n",
      "rmse = 39721.4348\n",
      "mape = 0.1029\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBRegressor()\n",
    "XGB.fit(x_train, y_train)\n",
    "\n",
    "pred = XGB.predict(x_test)\n",
    "regression_report(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [XGBoost 官方文檔](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\n",
    "\n",
    "## 計算 XGBoost 超參數搜尋時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HRSCV ended in 122.0842s\n",
      "\n",
      "Best score is 0.1006\n",
      "Best params is {'subsample': 0.76, 'objective': 'reg:squarederror', 'n_estimators': 192, 'min_child_weight': 0.6370665120808605, 'max_depth': 5, 'lambda': 0.56, 'eta': 0.02573989947635369, 'alpha': 0.41000000000000003}\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBRegressor(n_jobs = -1)\n",
    "\n",
    "# set searching hyperparameters\n",
    "search_params = {\n",
    "    'n_estimators': np.arange(70, 200),\n",
    "    'max_depth': np.arange(4, 8),\n",
    "    'eta': np.abs(norm(loc = 0.3, scale = 0.5).rvs(size=40)),\n",
    "    'objective': ['reg:squarederror', 'reg:squaredlogerror', 'reg:pseudohubererror'],\n",
    "    'min_child_weight': np.abs(norm(loc = 1, scale = 0.5).rvs(size=40)),\n",
    "    'subsample': np.arange(0.7, 1, 0.01),\n",
    "    'lambda': np.arange(0.01, 1, 0.05),\n",
    "    'alpha': np.arange(0.01, 1, 0.05)\n",
    "}\n",
    "\n",
    "# set Successive Halving algorithm\n",
    "SH_search = HRSCV(\n",
    "    estimator = XGB, param_distributions = search_params, n_candidates = 200, \n",
    "    factor = 2, resource = 'n_samples', max_resources='auto', min_resources='smallest', \n",
    "    aggressive_elimination=False, cv=3, scoring='neg_mean_absolute_percentage_error', refit=True,\n",
    "    return_train_score=True, random_state=None, n_jobs=-1, verbose=0\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "SH_search.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "print(f'HRSCV ended in {end - start:.4f}s\\n')\n",
    "\n",
    "print(f'Best score is {(-1) * SH_search.best_score_:.4f}')\n",
    "print(f'Best params is {SH_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算 XGBoost 單輪訓練時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost was trained in 0.2171s\n",
      "\n",
      "mse = 1010869446.2126\n",
      "mae = 15842.4891\n",
      "rmse = 31794.1731\n",
      "mape = 0.0904\n"
     ]
    }
   ],
   "source": [
    "# 這組參數是我搜尋到比較好的一組參數\n",
    "best_params = {'subsample': 0.71, 'objective': 'reg:squarederror', 'n_estimators': 135, 'min_child_weight': 1.106778500318303, 'max_depth': 4, 'lambda': 0.66, 'eta': 0.06554882531678949, 'alpha': 0.81}\n",
    "best_params['n_jobs'] = -1\n",
    "XGB = get_XGBoost(best_params)\n",
    "\n",
    "start = time.time()\n",
    "XGB.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "print(f'XGBoost was trained in {end - start:.4f}s\\n')\n",
    "\n",
    "pred = XGB.predict(x_test)\n",
    "regression_report(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算 Random Forest 超參數搜尋時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor(n_jobs = -1)\n",
    "\n",
    "# set searching hyperparameters\n",
    "search_params = {\n",
    "    'n_estimators': np.arange(70, 200),\n",
    "    'max_depth': np.arange(7, 20),\n",
    "    'ccp_alpha': np.abs(norm(loc = 1.5, scale = 0.5).rvs(size=20)),\n",
    "    'criterion': ['mae', 'mse'],\n",
    "    'min_samples_split': np.arange(2, 10),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_impurity_decrease': np.abs(norm(loc = 1, scale = 0.5).rvs(size=40)),\n",
    "    'max_samples': np.arange(0.7, 0.99, 0.01)\n",
    "}\n",
    "\n",
    "# set Successive Halving algorithm\n",
    "SH_search = HRSCV(\n",
    "    estimator = RF, param_distributions = search_params, n_candidates = 200, \n",
    "    factor = 2, resource = 'n_samples', max_resources='auto', min_resources='smallest', \n",
    "    aggressive_elimination=False, cv=3, scoring='neg_mean_absolute_percentage_error', refit=True,\n",
    "    return_train_score=True, random_state=None, n_jobs = -1, verbose=0\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "SH_search.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "print(f'HRSCV ended in {end - start:.4f}s\\n')\n",
    "\n",
    "print(f'Best score is {(-1) * SH_search.best_score_:.4f}')\n",
    "print(f'Best params is {SH_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算 Random Forest 單輪訓練時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest was trained in 0.1777s\n",
      "\n",
      "mse = 1192438196.9586\n",
      "mae = 18699.1861\n",
      "rmse = 34531.6984\n",
      "mape = 0.1060\n"
     ]
    }
   ],
   "source": [
    "best_params = {'n_estimators': 107, 'min_samples_split': 3, 'min_impurity_decrease': 1.5204750662278061, 'max_samples': 0.81, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'mse', 'ccp_alpha': 1.205496871022525}\n",
    "best_params['n_jobs'] = 4\n",
    "\n",
    "RF = RandomForestRegressor(**best_params)\n",
    "start = time.time()\n",
    "RF.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "print(f'Random Forest was trained in {end - start:.4f}s\\n')\n",
    "\n",
    "pred = RF.predict(x_test)\n",
    "regression_report(y_test, pred, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
